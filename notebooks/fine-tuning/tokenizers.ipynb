{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c4e38f5-7a6f-4491-993c-0d5168cccdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Package version: 0.0.post1.dev3+g8362a6c.d20240713\n",
      "PyTorch version: 2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "from pprint import pprint\n",
    "import textwrap\n",
    "\n",
    "# Appearance of the Notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Hugging Face \n",
    "from transformers import pipeline, set_seed, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Scikit-learn performance metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, f1_score \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import auc as calculate_auc\n",
    "\n",
    "# Import this module with autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import transformermodels as tm\n",
    "print(f'Package version: {tm.__version__}')\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7a29ad2-c178-4348-ae33-847ec0d20d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs found:  1\n",
      "Current device ID:     0\n",
      "GPU device name:       NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "CUDNN version:         8902\n",
      "\n",
      "Device for model training/inference: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# GPU checks\n",
    "is_cuda = torch.cuda.is_available()\n",
    "print(f'CUDA available: {is_cuda}')\n",
    "print(f'Number of GPUs found:  {torch.cuda.device_count()}')\n",
    "\n",
    "if is_cuda:\n",
    "    print(f'Current device ID:     {torch.cuda.current_device()}')\n",
    "    print(f'GPU device name:       {torch.cuda.get_device_name(0)}')\n",
    "    print(f'CUDNN version:         {torch.backends.cudnn.version()}')\n",
    "    device_str = 'cuda:0'\n",
    "    torch.cuda.empty_cache() \n",
    "else:\n",
    "    device_str = 'cpu'\n",
    "device = torch.device(device_str)\n",
    "print()\n",
    "print(f'Device for model training/inference: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ffbf9236-736e-440a-beed-e16a13af8d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions and parameters\n",
    "def wrap(x):\n",
    "    return textwrap.fill(x, replace_whitespace=False, fix_sentence_endings=True)\n",
    "\n",
    "# Directories\n",
    "data_dir = os.path.join(os.environ.get('HOME'), 'data', 'transformers')\n",
    "\n",
    "# Model checkpoint\n",
    "checkpoint = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1d272292-7bfd-487e-9812-8a9c6a39ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e47f845-c0a4-4d8a-9b1a-c78d54fc1ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0ef8a3a-98dd-481b-b07b-54940937ed78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 2088, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tokenizer('hello world'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7663c01e-3d13-47a3-91fd-8ccf7c620cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world']\n",
      "[7592, 2088]\n",
      "['hello', 'world']\n",
      "hello world\n",
      "[101, 7592, 2088, 102]\n",
      "[CLS] hello world [SEP]\n"
     ]
    }
   ],
   "source": [
    "input_str = 'hello world'\n",
    "# Create tokens from input\n",
    "tokens = tokenizer.tokenize(input_str)\n",
    "print(tokens)\n",
    "# Convert tokens to integet IDs\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "# We can also convert the ids back to a list of tokens\n",
    "print(tokenizer.convert_ids_to_tokens(ids))\n",
    "# We can also create the text from the ids\n",
    "print(tokenizer.decode(ids))\n",
    "# Encode\n",
    "ids = tokenizer.encode(input_str)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d3131bf8-a4c5-491b-aee4-6e3d20072de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7592, 2088, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 2066, 8870, 1012, 102], [101, 2079, 2017, 2066, 8870, 2205, 1029, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create model inputs\n",
    "model_inputs = tokenizer(input_str)\n",
    "print(model_inputs)\n",
    "\n",
    "# Tokenize multiple sentences at the same time\n",
    "data = ['I like cats.',\n",
    "        'Do you like cats too?']\n",
    "print()\n",
    "display(tokenizer(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e76f66af-7f37-4df8-84ef-84d40e95159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e4e89ea8-eb1e-4c3d-8be4-038a7fedcb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 2088,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create some outputs\n",
    "model_inputs = tokenizer(input_str, return_tensors='pt')\n",
    "display(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b295cd04-cc14-4b51-8612-43143f57183b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3974,  0.5378]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = model(**model_inputs)\n",
    "display(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a8109f08-43b6-4722-b56e-90b7e5ab82c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create a new model with three outputs\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41cde23d-2367-4e8f-9dfb-f665df89cc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2439, -0.0344,  0.0288]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.24388239 -0.03437275  0.02879009]]\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**model_inputs)\n",
    "display(outputs)\n",
    "print()\n",
    "print(outputs.get('logits').detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "119915b4-1943-4e69-894b-b2986fcb0c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize multiple sentences at the same time\n",
    "#data = ['I like cats.',\n",
    "#        'Do you like cats too?']\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=checkpoint, device=device)\n",
    "\n",
    "data = ['I like cats.',\n",
    "        'Do you like cats too?']\n",
    "\n",
    "model_inputs = tokenizer(data, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5b4d99f4-3cd0-416c-829e-12454e654bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1045, 2066, 8870, 1012,  102,    0,    0],\n",
       "        [ 101, 2079, 2017, 2066, 8870, 2205, 1029,  102]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model_inputs.get('input_ids'))\n",
    "display(model_inputs.get('attention_mask'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6e152118-5929-4556-a9b0-893de0b0693f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4125, -0.1180,  0.0774],\n",
      "        [-0.3841, -0.1545,  0.0242]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**model_inputs)\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
