{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aecc66c5-44be-44a3-91a8-c1a42b9bd024",
   "metadata": {},
   "source": [
    "### Fine-Tuning for Sentiment Analysis ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8c40f19-16e7-417e-9e33-d0b4a2b9d964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Package version: 0.0.post1.dev8+gd983d5a.d20240721\n",
      "PyTorch version: 2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "from pprint import pprint\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "# Appearance of the Notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "# Hugging Face \n",
    "from transformers import pipeline, set_seed, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# This HuggingFace community-driven open-source library of datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# Import this module with autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import transformermodels as tm\n",
    "print(f'Package version: {tm.__version__}')\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a29ad2-c178-4348-ae33-847ec0d20d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs found:  1\n",
      "Current device ID:     0\n",
      "GPU device name:       NVIDIA GeForce GTX 1080 with Max-Q Design\n",
      "CUDNN version:         8902\n",
      "\n",
      "Device for model training/inference: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# GPU checks\n",
    "is_cuda = torch.cuda.is_available()\n",
    "print(f'CUDA available: {is_cuda}')\n",
    "print(f'Number of GPUs found:  {torch.cuda.device_count()}')\n",
    "\n",
    "if is_cuda:\n",
    "    print(f'Current device ID:     {torch.cuda.current_device()}')\n",
    "    print(f'GPU device name:       {torch.cuda.get_device_name(0)}')\n",
    "    print(f'CUDNN version:         {torch.backends.cudnn.version()}')\n",
    "    device_str = 'cuda:0'\n",
    "    torch.cuda.empty_cache() \n",
    "else:\n",
    "    device_str = 'cpu'\n",
    "device = torch.device(device_str)\n",
    "print()\n",
    "print(f'Device for model training/inference: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ffbf9236-736e-440a-beed-e16a13af8d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions and parameters\n",
    "def wrap(x):\n",
    "    return textwrap.fill(x, replace_whitespace=False, fix_sentence_endings=True)\n",
    "\n",
    "# Directories\n",
    "data_dir = os.path.join(os.environ.get('HOME'), 'data', 'transformers')\n",
    "Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "model_dir = os.path.join(data_dir, 'model_trained', 'sentiment')\n",
    "Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "# Load the HuggingFace datasets\n",
    "# Full list of datasets\n",
    "# https://huggingface.co/datasets\n",
    "# dataset = load_dataset('amazon_polarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "026c0b39-5b52-4c14-b096-880df7a9054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 'glue' with subtastk 'sst2'\n",
    "raw_datasets = load_dataset('glue', 'sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35dcd038-4f0d-41c5-9340-0d063350ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'label', 'idx'],\n",
      "    num_rows: 67349\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets.get('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d000fcdd-2c57-42d4-b34f-57f03b017172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_TF_DATASET_REFS',\n",
      " '__class__',\n",
      " '__del__',\n",
      " '__delattr__',\n",
      " '__dict__',\n",
      " '__dir__',\n",
      " '__doc__',\n",
      " '__enter__',\n",
      " '__eq__',\n",
      " '__exit__',\n",
      " '__format__',\n",
      " '__ge__',\n",
      " '__getattribute__',\n",
      " '__getitem__',\n",
      " '__getitems__',\n",
      " '__getstate__',\n",
      " '__gt__',\n",
      " '__hash__',\n",
      " '__init__',\n",
      " '__init_subclass__',\n",
      " '__iter__',\n",
      " '__le__',\n",
      " '__len__',\n",
      " '__lt__',\n",
      " '__module__',\n",
      " '__ne__',\n",
      " '__new__',\n",
      " '__reduce__',\n",
      " '__reduce_ex__',\n",
      " '__repr__',\n",
      " '__setattr__',\n",
      " '__setstate__',\n",
      " '__sizeof__',\n",
      " '__str__',\n",
      " '__subclasshook__',\n",
      " '__weakref__',\n",
      " '_build_local_temp_path',\n",
      " '_check_index_is_initialized',\n",
      " '_data',\n",
      " '_estimate_nbytes',\n",
      " '_fingerprint',\n",
      " '_format_columns',\n",
      " '_format_kwargs',\n",
      " '_format_type',\n",
      " '_generate_tables_from_cache_file',\n",
      " '_generate_tables_from_shards',\n",
      " '_get_cache_file_path',\n",
      " '_get_output_signature',\n",
      " '_getitem',\n",
      " '_indexes',\n",
      " '_indices',\n",
      " '_info',\n",
      " '_map_single',\n",
      " '_new_dataset_with_indices',\n",
      " '_output_all_columns',\n",
      " '_push_parquet_shards_to_hub',\n",
      " '_save_to_disk_single',\n",
      " '_select_contiguous',\n",
      " '_select_with_indices_mapping',\n",
      " '_split',\n",
      " 'add_column',\n",
      " 'add_elasticsearch_index',\n",
      " 'add_faiss_index',\n",
      " 'add_faiss_index_from_external_arrays',\n",
      " 'add_item',\n",
      " 'align_labels_with_mapping',\n",
      " 'builder_name',\n",
      " 'cache_files',\n",
      " 'cast',\n",
      " 'cast_column',\n",
      " 'citation',\n",
      " 'class_encode_column',\n",
      " 'cleanup_cache_files',\n",
      " 'column_names',\n",
      " 'config_name',\n",
      " 'data',\n",
      " 'dataset_size',\n",
      " 'description',\n",
      " 'download_checksums',\n",
      " 'download_size',\n",
      " 'drop_index',\n",
      " 'export',\n",
      " 'features',\n",
      " 'filter',\n",
      " 'flatten',\n",
      " 'flatten_indices',\n",
      " 'format',\n",
      " 'formatted_as',\n",
      " 'from_buffer',\n",
      " 'from_csv',\n",
      " 'from_dict',\n",
      " 'from_file',\n",
      " 'from_generator',\n",
      " 'from_json',\n",
      " 'from_list',\n",
      " 'from_pandas',\n",
      " 'from_parquet',\n",
      " 'from_polars',\n",
      " 'from_spark',\n",
      " 'from_sql',\n",
      " 'from_text',\n",
      " 'get_index',\n",
      " 'get_nearest_examples',\n",
      " 'get_nearest_examples_batch',\n",
      " 'homepage',\n",
      " 'info',\n",
      " 'is_index_initialized',\n",
      " 'iter',\n",
      " 'license',\n",
      " 'list_indexes',\n",
      " 'load_elasticsearch_index',\n",
      " 'load_faiss_index',\n",
      " 'load_from_disk',\n",
      " 'map',\n",
      " 'num_columns',\n",
      " 'num_rows',\n",
      " 'prepare_for_task',\n",
      " 'push_to_hub',\n",
      " 'remove_columns',\n",
      " 'rename_column',\n",
      " 'rename_columns',\n",
      " 'reset_format',\n",
      " 'save_faiss_index',\n",
      " 'save_to_disk',\n",
      " 'search',\n",
      " 'search_batch',\n",
      " 'select',\n",
      " 'select_columns',\n",
      " 'set_format',\n",
      " 'set_transform',\n",
      " 'shape',\n",
      " 'shard',\n",
      " 'shuffle',\n",
      " 'size_in_bytes',\n",
      " 'skip',\n",
      " 'sort',\n",
      " 'split',\n",
      " 'supervised_keys',\n",
      " 'take',\n",
      " 'task_templates',\n",
      " 'to_csv',\n",
      " 'to_dict',\n",
      " 'to_iterable_dataset',\n",
      " 'to_json',\n",
      " 'to_list',\n",
      " 'to_pandas',\n",
      " 'to_parquet',\n",
      " 'to_polars',\n",
      " 'to_sql',\n",
      " 'to_tf_dataset',\n",
      " 'train_test_split',\n",
      " 'unique',\n",
      " 'version',\n",
      " 'with_format',\n",
      " 'with_transform']\n"
     ]
    }
   ],
   "source": [
    "# The DIR functions shows all of the methods of the object\n",
    "pprint(dir(raw_datasets.get('train')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6013623-9585-4e21-9737-8e2bc8197e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67344</th>\n",
       "      <td>a delightful comedy</td>\n",
       "      <td>1</td>\n",
       "      <td>67344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67345</th>\n",
       "      <td>anguish , anger and frustration</td>\n",
       "      <td>0</td>\n",
       "      <td>67345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67346</th>\n",
       "      <td>at achieving the modest , crowd-pleasing goals...</td>\n",
       "      <td>1</td>\n",
       "      <td>67346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67347</th>\n",
       "      <td>a patient viewer</td>\n",
       "      <td>1</td>\n",
       "      <td>67347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67348</th>\n",
       "      <td>this new jangle of noise , mayhem and stupidit...</td>\n",
       "      <td>0</td>\n",
       "      <td>67348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67349 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label    idx\n",
       "0           hide new secretions from the parental units       0      0\n",
       "1                   contains no wit , only labored gags       0      1\n",
       "2      that loves its characters and communicates som...      1      2\n",
       "3      remains utterly satisfied to remain the same t...      0      3\n",
       "4      on the worst revenge-of-the-nerds clichés the ...      0      4\n",
       "...                                                  ...    ...    ...\n",
       "67344                               a delightful comedy       1  67344\n",
       "67345                   anguish , anger and frustration       0  67345\n",
       "67346  at achieving the modest , crowd-pleasing goals...      1  67346\n",
       "67347                                  a patient viewer       1  67347\n",
       "67348  this new jangle of noise , mayhem and stupidit...      0  67348\n",
       "\n",
       "[67349 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets.get('train').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9de6b3b-e603-40fc-ac09-de8b3e0bc330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(raw_datasets.get('train')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89471b85-dec0-478d-b18b-d23583e19ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'yet this grating showcase ', 'label': 0, 'idx': 102}\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets.get('train')[102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5f7e2e6-0d77-4b60-b520-7428edf2ebd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': ['in memory ',\n",
       "  'respectable new one ',\n",
       "  'yet this grating showcase '],\n",
       " 'label': [1, 1, 0],\n",
       " 'idx': [100, 101, 102]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(raw_datasets.get('train')[100:103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68ced909-192f-4300-91a2-3a3a0eddb503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['negative', 'positive'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(raw_datasets.get('train').features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2b51fb1-c54f-4cf7-a822-7e717a86596c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 102],\n",
      "               [101,\n",
      "                3397,\n",
      "                2053,\n",
      "                15966,\n",
      "                1010,\n",
      "                2069,\n",
      "                4450,\n",
      "                2098,\n",
      "                18201,\n",
      "                2015,\n",
      "                102],\n",
      "               [101,\n",
      "                2008,\n",
      "                7459,\n",
      "                2049,\n",
      "                3494,\n",
      "                1998,\n",
      "                10639,\n",
      "                2015,\n",
      "                2242,\n",
      "                2738,\n",
      "                3376,\n",
      "                2055,\n",
      "                2529,\n",
      "                3267,\n",
      "                102]]}\n"
     ]
    }
   ],
   "source": [
    "# AutoTokenizer\n",
    "# checkpoint = 'bert-base-uncased'\n",
    "checkpoint = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Test the tokenizer\n",
    "tokenized_sentences = tokenizer(raw_datasets.get('train')[:3].get('sentence'))\n",
    "pprint(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0986c15-1d59-4967-9006-bc07f9ae9bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to pass the truncation argument\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch.get('sentence'), truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d85e61a1-92e3-4884-aeff-16de7d72f86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e253b3-ba0a-456c-acfe-20d2099d2e7c",
   "metadata": {},
   "source": [
    "### Model training for text sequence classification ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ac4a8c2-9af3-478d-b8de-d3f6fdf623a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments('my_trainer',\n",
    "                                  eval_strategy='epoch',\n",
    "                                  save_strategy='epoch',\n",
    "                                  num_train_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7d054c6-6e10-494a-a44d-545d0abdf054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and model checkpoint\n",
    "# checkpoint = 'bert-base-uncased'\n",
    "checkpoint = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b4d1170-3bf5-4f37-b0fb-2b40054f7c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DistilBertForSequenceClassification                     --\n",
       "├─DistilBertModel: 1-1                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              23,440,896\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─LayerNorm: 3-3                              1,536\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             42,527,232\n",
       "├─Linear: 1-2                                           590,592\n",
       "├─Linear: 1-3                                           1,538\n",
       "├─Dropout: 1-4                                          --\n",
       "================================================================================\n",
       "Total params: 66,955,010\n",
       "Trainable params: 66,955,010\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The AutoModelForSequnceClassification returns many types of models\n",
    "# Depending on the checkpoint\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2) \n",
    "print(type(model))\n",
    "display(summary(model))\n",
    "# pprint(model)\n",
    "# OUTPUT:(classifier): Linear(in_features=768, out_features=2, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c12e90e5-9574-4940-8ede-c2f9f7d25a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight\n",
      "tensor([[-0.0283, -0.0414,  0.0004,  ..., -0.0333, -0.0190,  0.0438],\n",
      "        [ 0.0440,  0.0149,  0.0072,  ..., -0.0220,  0.0383,  0.0030],\n",
      "        [-0.0457, -0.0289,  0.0271,  ...,  0.0017,  0.0291, -0.0178],\n",
      "        [ 0.0166, -0.0392, -0.0019,  ...,  0.0073, -0.0266, -0.0688],\n",
      "        [ 0.0041, -0.0368, -0.0095,  ..., -0.0111, -0.0342, -0.0254]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning: train all the weights in the network\n",
    "# We want to make sure that we are training all parameters\n",
    "# We will save all of the parameters before the training\n",
    "params_before = []\n",
    "params_before_dict = {}\n",
    "for name, p in model.named_parameters():\n",
    "    params_before.append(p.detach().cpu().numpy())\n",
    "    params_before_dict.update({name: p})\n",
    "print(len(params_before))\n",
    "# Lets take the parameters from one of the layers\n",
    "layer_name = list(params_before_dict.keys())[10]\n",
    "layer_p = params_before_dict.get(layer_name)\n",
    "\n",
    "print(layer_name)\n",
    "print(layer_p[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d0f52c6-29e5-4097-9c46-2e35d16ea481",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric('glue', 'sst2', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "987253b2-99f5-4841-9eda-6c0e99039777",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute(predictions=[1, 0, 1], references=[1, 0, 0])\n",
    "# We need to create a function for the metric so that we can use it in the training loop\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f306136c-ec35-429b-8a3d-61d5e356feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the trainier object\n",
    "trainer = Trainer(model=model, \n",
    "                  train_dataset=tokenized_datasets.get('train'),\n",
    "                  eval_dataset=tokenized_datasets.get('validation'),\n",
    "                  tokenizer=tokenizer,\n",
    "                  compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dd42d7b2-6b36-42ea-9598-6c3f3e87f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0f1380d0-98d1-4171-ae4a-c3ce666dc345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andreas/data/transformers/model_trained/sentiment/trained_sentiment\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_name = 'trained_sentiment'\n",
    "output_dir = os.path.join(model_dir, model_name)\n",
    "print(output_dir)\n",
    "# trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a760523-5f76-4373-b3b1-6ff694c95160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9999430179595947}]\n",
      "[{'label': 'LABEL_0', 'score': 0.9996452331542969}]\n"
     ]
    }
   ],
   "source": [
    "# Use the model\n",
    "newmodel = pipeline(task='text-classification', model=output_dir, device=device)\n",
    "print(newmodel('This is a great movie.'))\n",
    "print(newmodel('This movie is not so good.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3b95f-f2a4-4735-bb26-ded2f3930c50",
   "metadata": {},
   "source": [
    "### Change the output to the correct label names ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9f7b58b1-a5cb-4934-9924-de05c53b8a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': Value(dtype='string', id=None), 'label': ClassLabel(names=['negative', 'positive'], id=None), 'idx': Value(dtype='int32', id=None)}\n",
      "ClassLabel(names=['negative', 'positive'], id=None)\n",
      "\n",
      "['negative', 'positive']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_name_or_path': 'distilbert-base-uncased',\n",
       " 'activation': 'gelu',\n",
       " 'architectures': ['DistilBertForSequenceClassification'],\n",
       " 'attention_dropout': 0.1,\n",
       " 'dim': 768,\n",
       " 'dropout': 0.1,\n",
       " 'hidden_dim': 3072,\n",
       " 'initializer_range': 0.02,\n",
       " 'max_position_embeddings': 512,\n",
       " 'model_type': 'distilbert',\n",
       " 'n_heads': 12,\n",
       " 'n_layers': 6,\n",
       " 'pad_token_id': 0,\n",
       " 'problem_type': 'single_label_classification',\n",
       " 'qa_dropout': 0.1,\n",
       " 'seq_classif_dropout': 0.2,\n",
       " 'sinusoidal_pos_embds': False,\n",
       " 'tie_weights_': True,\n",
       " 'torch_dtype': 'float32',\n",
       " 'transformers_version': '4.42.4',\n",
       " 'vocab_size': 30522,\n",
       " 'id2label': {0: 'negative', 1: 'positive'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We get the label names from the training data sets\n",
    "print(raw_datasets.get('train').features)\n",
    "print(raw_datasets.get('train').features.get('label'))\n",
    "print()\n",
    "label_names = raw_datasets.get('train').features.get('label').names\n",
    "print(label_names)\n",
    "\n",
    "# Let's edit the config file of the trained model\n",
    "config_file = os.path.join(output_dir, 'config.json')\n",
    "\n",
    "# Open the config file\n",
    "with open(config_file) as fl:\n",
    "    js = json.load(fl)\n",
    "\n",
    "# Add the labels mappings\n",
    "js.update({'id2label': {0: 'negative', 1: 'positive'}})\n",
    "display(js)\n",
    "\n",
    "# Save the new configuration file\n",
    "with open(config_file, 'w') as fl:\n",
    "    json.dump(js, fl, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "111e95da-0995-43e8-a70c-5b38aaa8a839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new pipeline\n",
    "newmodel = pipeline(task='text-classification', model=output_dir, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fd0e2c66-3584-44a7-b52d-1a11947f1771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.9999401569366455}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel('This movie is great.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "af784dbc-e3df-4f73-93f6-7802e0558221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.0012, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.0012, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's confirm if the weights were changed during training\n",
    "params_after = []\n",
    "params_after_dict = {}\n",
    "for name, p in model.named_parameters():\n",
    "    params_after.append(p.detach().cpu().numpy())\n",
    "    params_after_dict.update({name: p})\n",
    "print(len(params_after))\n",
    "# Lets take the parameters from one of the layers\n",
    "layer_name_before = list(params_before_dict.keys())[10]\n",
    "layer_p_before = params_before_dict.get(layer_name)\n",
    "\n",
    "layer_name_after = list(params_after_dict.keys())[10]\n",
    "layer_p_after = params_after_dict.get(layer_name)\n",
    "\n",
    "print(layer_name_before)\n",
    "print(layer_name_after)\n",
    "\n",
    "display(layer_p_before[0, 5])\n",
    "display(layer_p_after[0, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c4526510-4f41-44fb-9936-67eb83cca85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24723.473\n",
      "141.43802\n",
      "2.8297658\n",
      "1.913683\n",
      "2169.042\n",
      "2.5821939\n",
      "2169.3225\n",
      "0.0056937574\n",
      "1967.1642\n",
      "1.6832343\n",
      "1885.6625\n",
      "1.2072973\n",
      "2.7268405\n",
      "1.1853302\n",
      "8225.339\n",
      "8.787317\n",
      "7659.0737\n",
      "1.0334922\n",
      "2.6662364\n",
      "1.2093645\n",
      "2132.775\n",
      "2.3922868\n",
      "2138.4028\n",
      "0.00527232\n",
      "1912.6146\n",
      "1.3291999\n",
      "1839.9622\n",
      "1.0666238\n",
      "2.6803317\n",
      "1.0824041\n",
      "8176.903\n",
      "8.3823\n",
      "7523.8022\n",
      "0.9768689\n",
      "2.447663\n",
      "1.1156534\n",
      "2109.289\n",
      "2.6267498\n",
      "2122.0803\n",
      "0.0048502535\n",
      "1885.4092\n",
      "1.1503797\n",
      "1853.506\n",
      "1.1245847\n",
      "2.7050886\n",
      "1.2291026\n",
      "8209.357\n",
      "8.93066\n",
      "7386.6978\n",
      "1.0882995\n",
      "2.425788\n",
      "0.9850048\n",
      "2080.8481\n",
      "2.2816079\n",
      "2083.7625\n",
      "0.0056488216\n",
      "1867.6721\n",
      "1.0481541\n",
      "1784.9973\n",
      "1.1396999\n",
      "2.4749026\n",
      "1.1881702\n",
      "8045.6313\n",
      "9.136528\n",
      "7007.7036\n",
      "1.2292962\n",
      "2.25871\n",
      "1.0910001\n",
      "1972.2903\n",
      "2.4923775\n",
      "1959.4954\n",
      "0.0031186645\n",
      "1639.4523\n",
      "1.1057569\n",
      "1626.8718\n",
      "1.6985041\n",
      "2.2145581\n",
      "1.6097157\n",
      "7267.7744\n",
      "8.912457\n",
      "5950.014\n",
      "1.595479\n",
      "2.35741\n",
      "1.9419782\n",
      "1825.0217\n",
      "2.106335\n",
      "1849.2722\n",
      "0.002102476\n",
      "1534.1357\n",
      "2.6422424\n",
      "1497.6665\n",
      "1.856708\n",
      "2.5620012\n",
      "2.2382953\n",
      "6438.0845\n",
      "9.444357\n",
      "5286.92\n",
      "1.675566\n",
      "3.5083866\n",
      "1.1125644\n",
      "1393.4172\n",
      "1.7999661\n",
      "4.1959805\n",
      "0.0019360104\n"
     ]
    }
   ],
   "source": [
    "# Compute the sum of the absolute differences between the untrained and trained model parameters\n",
    "for p1, p2 in zip(params_before, params_after):\n",
    "    print(np.sum(np.abs(p1 - p2)))\n",
    "\n",
    "# This output confirms that all of the model weights were updated during the training process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
