{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd16dd6e-8a2b-42f7-a2bc-d7f187e4514d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Package version: 0.0.post1.dev3+g8362a6c.d20240713\n",
      "PyTorch version: 2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "from pprint import pprint\n",
    "import textwrap\n",
    "import pickle\n",
    "\n",
    "# Appearance of the Notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Hugging Face \n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# Import this module with autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import transformermodels as tm\n",
    "print(f'Package version: {tm.__version__}')\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d798d945-43c7-43e6-a843-fa99f19a2631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs found:  1\n",
      "Current device ID:     0\n",
      "GPU device name:       NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "CUDNN version:         8902\n",
      "\n",
      "Device for model training/inference: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# GPU checks\n",
    "is_cuda = torch.cuda.is_available()\n",
    "print(f'CUDA available: {is_cuda}')\n",
    "print(f'Number of GPUs found:  {torch.cuda.device_count()}')\n",
    "\n",
    "if is_cuda:\n",
    "    print(f'Current device ID:     {torch.cuda.current_device()}')\n",
    "    print(f'GPU device name:       {torch.cuda.get_device_name(0)}')\n",
    "    print(f'CUDNN version:         {torch.backends.cudnn.version()}')\n",
    "    device_str = 'cuda:0'\n",
    "    torch.cuda.empty_cache() \n",
    "else:\n",
    "    device_str = 'cpu'\n",
    "device = torch.device(device_str)\n",
    "print()\n",
    "print(f'Device for model training/inference: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ce8706-ff64-4576-87e3-5ec179646905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def wrap(x):\n",
    "    return textwrap.fill(x, replace_whitespace=False, fix_sentence_endings=True)\n",
    "\n",
    "# Directories\n",
    "data_dir = os.path.join(os.environ.get('HOME'), 'data', 'transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90c7e75-6a4e-4a87-a268-b12e2ea06078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d92c5152f4408b93ae2584f8c7f0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d018a84e0664e73bd704c1212f5dc68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a507cc4c5d41d381e42e67897597a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660aa0c301a84402961a5ca70a1ad4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner = pipeline(task='ner', aggregation_strategy='simple', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "535d136b-e166-4bd1-b49a-5585a6592717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981982,\n",
       "  'word': 'Steve Jobs',\n",
       "  'start': 0,\n",
       "  'end': 10},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.99860126,\n",
       "  'word': 'Apple',\n",
       "  'start': 26,\n",
       "  'end': 31},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99932134,\n",
       "  'word': 'California',\n",
       "  'start': 50,\n",
       "  'end': 60}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = 'Steve Jobs was the CEO of Apple, headquartered in California'\n",
    "display(ner(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccbb6c1b-7b13-4522-9d96-e953a15de136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data CONLL 2003, the most popular NER data set\n",
    "with open(os.path.join(data_dir, 'ner_train.pkl'), 'rb') as fl:\n",
    "    corpus_train = pickle.load(fl)\n",
    "with open(os.path.join(data_dir, 'ner_test.pkl'), 'rb') as fl:\n",
    "    corpus_test = pickle.load(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "004f2ddf-d4c5-491e-8145-69730314846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "corpus = corpus_test\n",
    "\n",
    "for sentence_tag_pairs in corpus:\n",
    "    tokens = []\n",
    "    target = []\n",
    "    for token, tag in sentence_tag_pairs:\n",
    "        tokens.append(token)\n",
    "        target.append(tag)\n",
    "    inputs.append(tokens)\n",
    "    targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaa66cb8-bb8f-4d80-ae47-81ce3b330ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'was', 'well', 'backed', 'by', 'England', 'hopeful', 'Mark', 'Butcher', 'who', 'made', '70', 'as', 'Surrey', 'closed', 'on', '429', 'for', 'seven', ',', 'a', 'lead', 'of', '234', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(inputs[9])\n",
    "print(targets[9])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
